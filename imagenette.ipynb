{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71997356",
   "metadata": {},
   "source": [
    "# Tarea 1 - Taller de Deep Learning\n",
    "\n",
    "**Johny Kidd: 228175**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596d8004",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7263d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Reproducibilidad\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Usando device:\", device)\n",
    "\n",
    "# Weights & Biases\n",
    "import wandb\n",
    "wandb.login() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d1eba",
   "metadata": {},
   "source": [
    "## Seteo de Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddb294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eba7048",
   "metadata": {},
   "source": [
    "# 1. Carga de Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a17f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((160, 160)), \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Cargamos Imagenette 160px\n",
    "train_dataset = datasets.Imagenette(\n",
    "    root=\"./data\", size=\"160px\", split=\"train\", download=False, transform=transform\n",
    ")\n",
    "val_dataset = datasets.Imagenette(\n",
    "    root=\"./data\", size=\"160px\", split=\"val\", download=False, transform=transform\n",
    ")\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Val size: {len(val_dataset)}\")\n",
    "print(f\"Clases: {train_dataset.classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01422780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar algunos ejemplos de imágenes\n",
    "def show_samples(dataset, n=8):\n",
    "    indices = np.random.choice(len(dataset), n, replace=False)  # Selecciona índices aleatorios\n",
    "    fig, axes = plt.subplots(1, n, figsize=(15, 3))\n",
    "    for i, idx in enumerate(indices):\n",
    "        img, label = dataset[idx]\n",
    "        axes[i].imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "        axes[i].set_title(dataset.classes[label], fontsize=8)\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "show_samples(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff1564d",
   "metadata": {},
   "source": [
    "# 3. Análisis Exploratorio de Datos (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f411fae",
   "metadata": {},
   "source": [
    "## Balanceo de Clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78d2808",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_dataset.classes\n",
    "class_names_simple = [c[0] if isinstance(c, tuple) else c for c in class_names]\n",
    "\n",
    "train_labels = [label for _, label in train_dataset._samples]\n",
    "val_labels = [label for _, label in val_dataset._samples]\n",
    "\n",
    "train_counts = Counter(train_labels)\n",
    "val_counts = Counter(val_labels)\n",
    "\n",
    "train_distribution = [train_counts[i] for i in range(len(class_names))]\n",
    "val_distribution = [val_counts[i] for i in range(len(class_names))]\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(x=class_names_simple, y=train_distribution, color=\"skyblue\", label=\"Train\")\n",
    "sns.barplot(x=class_names_simple, y=val_distribution, color=\"salmon\", alpha=0.7, label=\"Val\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Distribución de imágenes por clase\")\n",
    "plt.ylabel(\"Cantidad de imágenes\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540985ac",
   "metadata": {},
   "source": [
    "## Tamaño y Aspect Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7657729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "widths, heights, aspect_ratios = [], [], []\n",
    "\n",
    "\n",
    "for img, _ in train_dataset:\n",
    "    pil_img = transforms.ToPILImage()(img)\n",
    "    w, h = pil_img.size  \n",
    "    widths.append(w)\n",
    "    heights.append(h)\n",
    "    aspect_ratios.append(w / h)\n",
    "\n",
    "widths = np.array(widths)\n",
    "heights = np.array(heights)\n",
    "aspect_ratios = np.array(aspect_ratios)\n",
    "\n",
    "print(\"Ancho: mean={:.1f}, std={:.1f}, min={}, max={}\".format(widths.mean(), widths.std(), widths.min(), widths.max()))\n",
    "print(\"Alto: mean={:.1f}, std={:.1f}, min={}, max={}\".format(heights.mean(), heights.std(), heights.min(), heights.max()))\n",
    "print(\"Aspect ratio: mean={:.2f}, std={:.2f}, min={:.2f}, max={:.2f}\".format(aspect_ratios.mean(), aspect_ratios.std(), aspect_ratios.min(), aspect_ratios.max()))\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.hist(widths, bins=30, color='skyblue')\n",
    "plt.title(\"Distribución de ancho\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist(heights, bins=30, color='salmon')\n",
    "plt.title(\"Distribución de alto\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.hist(aspect_ratios, bins=30, color='lightgreen')\n",
    "plt.title(\"Distribución de aspect ratio\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d62061",
   "metadata": {},
   "source": [
    "Todas las imágenes del dataset han sido redimensionadas a un tamaño uniforme de 160x160 píxeles, con un aspect ratio de 1.0. Esto garantiza que el modelo reciba entradas homogéneas, facilitando el procesamiento por lotes y evitando distorsiones o sesgos relacionados con diferentes tamaños o proporciones. Esta uniformidad es fundamental para asegurar un entrenamiento estable y eficiente, y permite comparar resultados de manera justa entre diferentes experimentos y arquitecturas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d159776",
   "metadata": {},
   "source": [
    "## Media y Desviación Estándar de canales de color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e2c243",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "stds = []\n",
    "\n",
    "#iteramos por batchss para no sarturar memoria\n",
    "loader = DataLoader(train_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "for imgs, _ in loader:\n",
    "    means.append(imgs.mean(dim=[0,2,3]))  # mean por canal\n",
    "    stds.append(imgs.std(dim=[0,2,3]))    # std por canal\n",
    "\n",
    "mean = torch.stack(means).mean(dim=0)\n",
    "std = torch.stack(stds).mean(dim=0)\n",
    "\n",
    "print(\"Media por canal RGB:\", mean)\n",
    "print(\"Desviación estándar por canal RGB:\", std)\n",
    "\n",
    "# Histograma de intensidad por canal\n",
    "all_pixels = []\n",
    "for imgs, _ in loader:\n",
    "    all_pixels.append(imgs)\n",
    "\n",
    "all_pixels = torch.cat(all_pixels, dim=0)  # shape [N, C, H, W]\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "colors = ['r','g','b']\n",
    "for i, color in enumerate(colors):\n",
    "    plt.hist(all_pixels[:,i,:,:].numpy().flatten(), bins=50, color=color, alpha=0.5, label=f'Canal {color.upper()}')\n",
    "plt.title(\"Histograma de intensidad por canal RGB\")\n",
    "plt.xlabel(\"Valor de píxel\")\n",
    "plt.ylabel(\"Cantidad\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a83211",
   "metadata": {},
   "source": [
    "## Calidads de las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ac250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Definición de Funciones de Puntuación ---\n",
    "def blur_score(img):\n",
    "    \"\"\"Calcula el score de borrosidad usando la Varianza del Laplaciano.\"\"\"\n",
    "    img_gray = np.array(img.convert('L'))\n",
    "    return cv2.Laplacian(img_gray, cv2.CV_64F).var()\n",
    "\n",
    "def brightness_score(img):\n",
    "    \"\"\"Calcula el brillo promedio (media del píxel) en escala de grises.\"\"\"\n",
    "    img_gray = np.array(img.convert('L'))\n",
    "    return img_gray.mean()\n",
    "\n",
    "def contrast_score(img):\n",
    "    \"\"\"Calcula el contraste (desviación estándar) en escala de grises.\"\"\"\n",
    "    img_gray = np.array(img.convert('L'))\n",
    "    return img_gray.std()\n",
    "\n",
    "# --- Umbrales (los que definiste) ---\n",
    "blur_threshold = 10      # <10: borrosa\n",
    "dark_threshold = 30      # <30: muy oscura\n",
    "bright_threshold = 220   # >220: muy brillante\n",
    "contrast_threshold = 15  # <15: poco contraste\n",
    "\n",
    "# --- Lógica de Conteo Detallado ---\n",
    "total_images = len(train_dataset)\n",
    "\n",
    "# Contadores para cada tipo de problema (pueden superponerse)\n",
    "count_blur = 0\n",
    "count_dark = 0\n",
    "count_bright = 0\n",
    "count_low_contrast = 0\n",
    "count_multiple_problems = 0\n",
    "total_problem_count = 0\n",
    "\n",
    "problem_images_examples = [] # Para guardar un máximo de 20 ejemplos para plotear\n",
    "\n",
    "print(f\"Iniciando el análisis de calidad detallado en {total_images} imágenes...\")\n",
    "\n",
    "# Iterar sobre todo el dataset de entrenamiento\n",
    "for i, (img, label) in enumerate(train_dataset):\n",
    "    # Asegurarse de tener una imagen PIL para los cálculos\n",
    "    pil_img = img if isinstance(img, Image.Image) else transforms.ToPILImage()(img)\n",
    "    \n",
    "    blur = blur_score(pil_img)\n",
    "    bright = brightness_score(pil_img)\n",
    "    contrast = contrast_score(pil_img)\n",
    "    \n",
    "    current_problems = 0\n",
    "    is_problematic = False\n",
    "    \n",
    "    # 1. Borrosidad\n",
    "    if blur < blur_threshold:\n",
    "        count_blur += 1\n",
    "        current_problems += 1\n",
    "        is_problematic = True\n",
    "\n",
    "    # 2. Brillo (Oscuro)\n",
    "    if bright < dark_threshold:\n",
    "        count_dark += 1\n",
    "        current_problems += 1\n",
    "        is_problematic = True\n",
    "\n",
    "    # 3. Brillo (Brillante)\n",
    "    if bright > bright_threshold:\n",
    "        count_bright += 1\n",
    "        current_problems += 1\n",
    "        is_problematic = True\n",
    "\n",
    "    # 4. Contraste (Bajo)\n",
    "    if contrast < contrast_threshold:\n",
    "        count_low_contrast += 1\n",
    "        current_problems += 1\n",
    "        is_problematic = True\n",
    "\n",
    "    # Contar si tiene MÚLTIPLES problemas (2 o más)\n",
    "    if current_problems >= 2:\n",
    "        count_multiple_problems += 1\n",
    "        \n",
    "    # Contar el total de imágenes con AL MENOS un problema\n",
    "    if is_problematic:\n",
    "        total_problem_count += 1\n",
    "        # Guardar ejemplos solo si no hemos alcanzado el límite de 20\n",
    "        if len(problem_images_examples) < 20:\n",
    "            problem_images_examples.append((pil_img, label, blur, bright, contrast))\n",
    "\n",
    "\n",
    "# --- Resultados del Conteo ---\n",
    "def get_percentage(count, total):\n",
    "    return (count / total) * 100\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total de imágenes analizadas: {total_images}\")\n",
    "print(\"\\n--- Desglose de Problemas (Superpuestos) ---\")\n",
    "print(f\"1. Borrosidad (<{blur_threshold:.1f}):      {count_blur} ({get_percentage(count_blur, total_images):.2f}%)\")\n",
    "print(f\"2. Muy Oscuras (<{dark_threshold:.1f}):    {count_dark} ({get_percentage(count_dark, total_images):.2f}%)\")\n",
    "print(f\"3. Muy Brillantes (>{bright_threshold:.1f}): {count_bright} ({get_percentage(count_bright, total_images):.2f}%)\")\n",
    "print(f\"4. Bajo Contraste (<{contrast_threshold:.1f}): {count_low_contrast} ({get_percentage(count_low_contrast, total_images):.2f}%)\")\n",
    "\n",
    "print(\"\\n--- Resumen General ---\")\n",
    "print(f\"Imágenes con MÚLTIPLES problemas (>=2): {count_multiple_problems} ({get_percentage(count_multiple_problems, total_images):.2f}%)\")\n",
    "print(f\"Imágenes problemáticas TOTALES (>=1):    {total_problem_count} ({get_percentage(total_problem_count, total_images):.2f}%)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Visualización de Ejemplos ---\n",
    "# (El código de visualización de ejemplos sigue siendo el mismo)\n",
    "if problem_images_examples:\n",
    "    num_plots = len(problem_images_examples)\n",
    "    rows = (num_plots + 4) // 5  # Calcular filas necesarias para 5 columnas\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, 5, figsize=(10, 3 * rows))\n",
    "    \n",
    "    # Manejar el caso de una sola fila de plots\n",
    "    axes_flat = axes.flatten() if isinstance(axes, np.ndarray) and axes.ndim > 1 else (axes.flatten() if isinstance(axes, np.ndarray) else [axes])\n",
    "    \n",
    "    for ax, (img, label, blur, bright, contrast) in zip(axes_flat, problem_images_examples):\n",
    "        # Asumiendo que 'class_names_simple' está definido. Si no, usa f\"Clase {label}\"\n",
    "        try:\n",
    "            class_name = class_names_simple[label]\n",
    "        except (NameError, TypeError):\n",
    "            class_name = f\"Clase {label}\"\n",
    "\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"{class_name}\\nB: {blur:.1f}, Br: {bright:.1f}, C: {contrast:.1f}\", fontsize=8)\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Ocultar ejes sobrantes\n",
    "    for i in range(num_plots, len(axes_flat)):\n",
    "        axes_flat[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No se encontraron ejemplos problemáticos para plotear con los umbrales definidos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caf60b1",
   "metadata": {},
   "source": [
    "## Brillo, Contraste y Saturación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dffd00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "\n",
    "brightness_list = []\n",
    "contrast_list = []\n",
    "saturation_list = []\n",
    "\n",
    "for img, _ in train_dataset:\n",
    "    # Convertir a PIL si no lo es\n",
    "    pil_img = img if isinstance(img, Image.Image) else transforms.ToPILImage()(img)\n",
    "    \n",
    "    # Convertir a array\n",
    "    img_arr = np.array(pil_img)\n",
    "    \n",
    "    # Brillo y contraste (en escala de grises)\n",
    "    gray = cv2.cvtColor(img_arr, cv2.COLOR_RGB2GRAY)\n",
    "    brightness_list.append(gray.mean())\n",
    "    contrast_list.append(gray.std())\n",
    "    \n",
    "    # Saturación promedio\n",
    "    hsv = cv2.cvtColor(img_arr, cv2.COLOR_RGB2HSV)\n",
    "    saturation_list.append(hsv[:,:,1].mean())\n",
    "\n",
    "# Convertir a numpy\n",
    "brightness_list = np.array(brightness_list)\n",
    "contrast_list = np.array(contrast_list)\n",
    "saturation_list = np.array(saturation_list)\n",
    "\n",
    "# Histogramas\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.hist(brightness_list, bins=50, color='gold', alpha=0.7)\n",
    "plt.title(\"Distribución de brillo\")\n",
    "plt.xlabel(\"Valor medio de píxel\")\n",
    "plt.ylabel(\"Cantidad de imágenes\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist(contrast_list, bins=50, color='lightblue', alpha=0.7)\n",
    "plt.title(\"Distribución de contraste\")\n",
    "plt.xlabel(\"Desviación estándar de píxeles\")\n",
    "plt.ylabel(\"Cantidad de imágenes\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.hist(saturation_list, bins=50, color='lightgreen', alpha=0.7)\n",
    "plt.title(\"Distribución de saturación\")\n",
    "plt.xlabel(\"Valor medio de saturación\")\n",
    "plt.ylabel(\"Cantidad de imágenes\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e69fa",
   "metadata": {},
   "source": [
    "## Análisis del espectro de frecuencia (FFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e65941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "def calcular_espectro_fft(img):\n",
    "    \"\"\"\n",
    "    Calcula el espectro de magnitud de Fourier centrado de una imagen RGB/PIL/torch.Tensor.\n",
    "    \"\"\"\n",
    "    # Convertir a PIL si es tensor\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        img = img.numpy().transpose(1, 2, 0)  # C,H,W -> H,W,C\n",
    "        img = (img * 255).astype(np.uint8)    # De [0,1] a [0,255]\n",
    "    elif isinstance(img, Image.Image):\n",
    "        img = np.array(img)\n",
    "    \n",
    "    # Convertir a escala de grises\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # FFT 2D y centrar frecuencia baja\n",
    "    f = np.fft.fft2(gray)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "    \n",
    "    # Magnitud logarítmica\n",
    "    magnitude = 20 * np.log(np.abs(fshift) + 1e-8)  # epsilon para evitar log(0)\n",
    "    return magnitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d90e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm \n",
    "import random\n",
    "\n",
    "num_samples = min(9000, len(train_dataset))  # limitar muestra por velocidad\n",
    "indices_muestra = random.sample(range(len(train_dataset)), num_samples)\n",
    "\n",
    "# Inicializar con la primera imagen\n",
    "espectro_acumulado = np.zeros_like(calcular_espectro_fft(train_dataset[0][0]))\n",
    "\n",
    "print(f\"Calculando espectro promedio sobre {num_samples} imágenes...\")\n",
    "for i in tqdm(indices_muestra):\n",
    "    img, _ = train_dataset[i]\n",
    "    espectro = calcular_espectro_fft(img)\n",
    "    if espectro.shape == espectro_acumulado.shape:\n",
    "        espectro_acumulado += espectro\n",
    "\n",
    "espectro_promedio = espectro_acumulado / num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3e353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "espectro_norm = (espectro_promedio - np.min(espectro_promedio)) / (np.max(espectro_promedio) - np.min(espectro_promedio))\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(espectro_norm, cmap='gray')\n",
    "plt.title(f'Espectro de Magnitud Promedio (N={num_samples})')\n",
    "plt.colorbar(label='Intensidad Normalizada (Log)')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6f5c2a",
   "metadata": {},
   "source": [
    "## Estimación de ruido local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd7f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "# Asumo que 'transforms' viene de PyTorch si usas un dataset estándar\n",
    "try:\n",
    "    import torchvision.transforms as transforms\n",
    "except ImportError:\n",
    "    # Definir un dummy si no se usa PyTorch (solo si no se requiere la conversión)\n",
    "    class DummyTransform:\n",
    "        def __call__(self, img):\n",
    "            return img\n",
    "    transforms = type('transforms', (object,), {'ToPILImage': lambda: DummyTransform()})()\n",
    "\n",
    "# Celda 1: Cálculo de Ruido y Almacenamiento de Índices\n",
    "\n",
    "# Parámetros para la estimación:\n",
    "PATCH_SIZE = 10 \n",
    "num_muestras_ruido = min(9000, len(train_dataset)) \n",
    "\n",
    "def estimar_ruido_local(image_rgb):\n",
    "    \"\"\"Estima el ruido como la desviación estándar local en la imagen.\"\"\"\n",
    "    \n",
    "    # Manejo del formato de la imagen para cv2\n",
    "    if image_rgb.dtype != np.uint8:\n",
    "        # Asume que si no es uint8, está normalizada a 0-1, la escalamos.\n",
    "        # Ajusta esta línea si sabes que tus arrays NumPy tienen otro rango (ej. 0-255 en float)\n",
    "        image_rgb = (image_rgb * 255).astype(np.uint8) \n",
    "        \n",
    "    gray_image = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2GRAY)\n",
    "    H, W = gray_image.shape\n",
    "    \n",
    "    if H < PATCH_SIZE or W < PATCH_SIZE:\n",
    "         return 0 # Si es muy pequeña, la omitimos o asignamos un valor bajo\n",
    "         \n",
    "    varianzas_locales = []\n",
    "    \n",
    "    # Tomar 10 parches aleatorios en la imagen para estimar la variación local\n",
    "    for _ in range(10): \n",
    "        x = np.random.randint(0, W - PATCH_SIZE)\n",
    "        y = np.random.randint(0, H - PATCH_SIZE)\n",
    "        \n",
    "        patch = gray_image[y:y + PATCH_SIZE, x:x + PATCH_SIZE]\n",
    "        varianzas_locales.append(np.std(patch))\n",
    "        \n",
    "    return np.mean(varianzas_locales)\n",
    "\n",
    "# Almacenamos tuplas (índice, ruido_estimado)\n",
    "ruidos_estimados_con_indice = []\n",
    "print(f\"Estimando ruido local en {num_muestras_ruido} imágenes...\")\n",
    "\n",
    "for i in tqdm(range(num_muestras_ruido)):\n",
    "    # Asume que train_dataset[i] devuelve (imagen, etiqueta)\n",
    "    img, _ = train_dataset[i] \n",
    "    \n",
    "    # Manejo de formatos: si es un objeto Tensor/PIL, lo convertimos a NumPy array\n",
    "    if not isinstance(img, np.ndarray):\n",
    "        try:\n",
    "            # Si es un Tensor de PyTorch, lo convertimos a NumPy (CHW -> HWC)\n",
    "            # Nota: Esto asume el orden de canales de PyTorch\n",
    "            img = np.array(transforms.ToPILImage()(img))\n",
    "        except:\n",
    "             # Si falla la conversión a PIL/NumPy (caso extremo), usamos el array directamente si es posible\n",
    "             if hasattr(img, 'numpy'):\n",
    "                 img = img.numpy().transpose((1, 2, 0))\n",
    "             else:\n",
    "                 img = np.array(img)\n",
    "    \n",
    "    ruido_est = estimar_ruido_local(img)\n",
    "    ruidos_estimados_con_indice.append((i, ruido_est))\n",
    "\n",
    "# Convertir a array NumPy para cálculos estadísticos\n",
    "ruidos_array = np.array([r[1] for r in ruidos_estimados_con_indice])\n",
    "media_ruido = np.mean(ruidos_array)\n",
    "std_ruido = np.std(ruidos_array)\n",
    "\n",
    "print(\"\\n--- Resultados Estadísticos ---\")\n",
    "print(f\"Media ($\\mu$): {media_ruido:.2f}, Desviación Estándar ($\\sigma$): {std_ruido:.2f}\")\n",
    "\n",
    "# --- Identificación de Outliers (Para la visualización) ---\n",
    "umbral_alto = media_ruido + 2 * std_ruido\n",
    "indices_ruido_alto = [\n",
    "    i for i, ruido in ruidos_estimados_con_indice if ruido > umbral_alto\n",
    "]\n",
    "\n",
    "print(f\"Umbral para ruido alto ($\\mu + 2\\sigma$): {umbral_alto:.2f}\")\n",
    "print(f\"Se encontraron {len(indices_ruido_alto)} imágenes con $\\sigma_n$ superior a este umbral.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5621cd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(ruidos_array, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(media_ruido, color='red', linestyle='dashed', linewidth=1, label=f'Media: {media_ruido:.2f}')\n",
    "plt.title('Distribución de la Estimación de Ruido Local ($\\sigma_n$)')\n",
    "plt.xlabel('Desviación Estándar Local (Estimación $\\sigma_n$)')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"El alto valor de la media (23.48) indica una alta granularidad/textura en el dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 3: Visualización de Ejemplos de Ruido Alto (Outliers)\n",
    "\n",
    "num_a_mostrar = min(10, len(indices_ruido_alto))\n",
    "indices_a_mostrar = indices_ruido_alto[:num_a_mostrar]\n",
    "\n",
    "if num_a_mostrar > 0:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.suptitle(f\"Ejemplos de Imágenes con Ruido Local Alto ($\\sigma_n > {umbral_alto:.2f}$)\", fontsize=16)\n",
    "\n",
    "    for i, idx_original in enumerate(indices_a_mostrar):\n",
    "        img, label_idx = train_dataset[idx_original]\n",
    "        \n",
    "        # Volvemos a convertir a NumPy si es necesario\n",
    "        if not isinstance(img, np.ndarray):\n",
    "            try:\n",
    "                img = np.array(transforms.ToPILImage()(img))\n",
    "            except:\n",
    "                 if hasattr(img, 'numpy'):\n",
    "                     img = img.numpy().transpose((1, 2, 0))\n",
    "                 else:\n",
    "                     img = np.array(img)\n",
    "\n",
    "        # Buscar el valor de ruido específico\n",
    "        ruido_especifico = [r[1] for r in ruidos_estimados_con_indice if r[0] == idx_original][0]\n",
    "        \n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(img) \n",
    "        plt.title(f\"Clase: {label_idx}\\n$\\\\sigma_n$: {ruido_especifico:.2f}\", fontsize=10)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay suficientes imágenes con ruido extremadamente alto (o el umbral es muy estricto) para mostrar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d507c1b3",
   "metadata": {},
   "source": [
    "# 4. Principales Insights del EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7956e61a",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Geometría y Distribución (Puntos 1 y 2)\n",
    "\n",
    "### Hallazgos Clave\n",
    "* **Balance de Clases:** El *dataset* está **perfectamente balanceado** (aprox. 950 imágenes por clase), sin ninguna clase minoritaria significativa.\n",
    "* **Uniformidad Geómetrica:** Todas las imágenes son de **$160 \\times 160$ píxeles** con una relación de aspecto cuadrada ($1.0$).\n",
    "\n",
    "### Decisiones de Preprocesamiento\n",
    "| Decisión | Justificación del EDA |\n",
    "| :--- | :--- |\n",
    "| **No se requiere *weighted loss*** | El balance perfecto de clases elimina la necesidad de técnicas de mitigación de desbalance (ej. SMOTE, pérdida ponderada). La **precisión** global es una métrica confiable. |\n",
    "| **Transformación de *Resize*** | Ya que todas las imágenes son uniformes ($160 \\times 160$), el *resize* no es un paso de corrección, sino de **adaptación** si se usa una arquitectura pre-entrenada (ej. a $224 \\times 224$ o $256 \\times 256$). |\n",
    "| **Random Resized Crop (RRC)** | Obligatorio, dado que la variación de escala es $0$. El RRC es esencial para introducir **variabilidad de tamaño** y **eliminar los posibles bordes no informativos** (picos en $0.0/1.0$ del histograma). |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79757e71",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Color y Brillo (Puntos 3, 4 y 5)\n",
    "\n",
    "### Hallazgos Clave\n",
    "* **Iluminación/Varianza:** La media RGB general ($\\mu \\approx 0.45$) indica una ligera oscuridad, pero la desviación estándar ($\\sigma \\approx 0.27$) revela una **alta varianza de contraste y brillo**.\n",
    "* **Picos Extremos:** El histograma RGB muestra picos fuertes en los valores $0.0$ y $1.0$ (especialmente en el canal Azul), indicando la presencia de **fondos negros/blancos y luces/sombras extremas**.\n",
    "* **Baja Saturación:** La distribución de saturación es **asimétrica**, concentrándose en valores bajos ($\\approx 50$), lo que implica que la mayoría de las imágenes tienen colores apagados o moderados.\n",
    "* **Calidad:** Solo el **$1.73\\%$** de las imágenes son problemáticas (extremos de brillo/contraste), por lo que **no se requiere limpieza manual**.\n",
    "\n",
    "### Decisiones de Preprocesamiento y Augmentation\n",
    "| Decisión | Justificación del EDA |\n",
    "| :--- | :--- |\n",
    "| **Normalización (RGB)** | Necesaria para centrar los datos y optimizar la convergencia del modelo. Se aplicará con los valores calculados: $\\mu=[0.4625, 0.4580, 0.4298]$ y $\\sigma=[0.2748, 0.2690, 0.2856]$. |\n",
    "| **Augmentation de Saturación** | **Prioridad Alta.** La baja saturación del *dataset* exige una **variación agresiva** (ej. rango $\\mathbf{(0.5, 1.5)}$) para evitar que el modelo se sobreajuste a los colores apagados. |\n",
    "| **Augmentation de Brillo/Contraste** | **Prioridad Media.** Necesario para mitigar el $1.73\\%$ de *outliers* extremos y manejar la alta varianza natural del contraste (ej. rango $\\mathbf{(0.7, 1.3)}$). |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a74ad6",
   "metadata": {},
   "source": [
    "## 3. Textura y Frecuencia (Puntos 6 y 7)\n",
    "\n",
    "### Hallazgos Clave\n",
    "* **FFT (Global):** La mayor energía se concentra en las **bajas frecuencias** (formas grandes, fondos uniformes), con líneas axiales que sugieren estructuras fuertes (horizontales/verticales) o artefactos de *resizing*.\n",
    "* **Ruido Local ($\\sigma_n$):** La media de la desviación estándar local es **alta** ($\\mu \\approx 23.4$), lo que indica una **rica y alta granularidad/textura** a nivel de parche.\n",
    "* **Outliers Texturales:** Se identificó un pequeño grupo de imágenes (aprox. $280$) con textura excepcionalmente alta ($\\sigma_n > 42.4$).\n",
    "\n",
    "### Decisiones de Modelado y Augmentation\n",
    "| Decisión | Justificación del EDA |\n",
    "| :--- | :--- |\n",
    "| **Augmentation de Ruido** | **Esencial.** Aplicar **Ruido Gaussiano aleatorio** con baja probabilidad, imitando el pequeño grupo de *outliers* de alta $\\sigma_n$, haciendo el modelo robusto a variaciones texturales y ruido ausente en el FFT. |\n",
    "| **Augmentation Geométrico** | **Necesario.** Incluir transformaciones como **rotaciones leves** o **shear** para forzar al modelo a centrarse en la **forma** del objeto (baja frecuencia) independientemente de su orientación, rompiendo la uniformidad axial del FFT. |\n",
    "| **Elección de Arquitectura** | La alta granularidad ($\\sigma_n$) sugiere que las arquitecturas basadas en convolución (ej. **ResNet, VGG**) serán efectivas, ya que sus filtros pequeños ($3 \\times 3$) están bien equipados para capturar la rica información textural local. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f71c16",
   "metadata": {},
   "source": [
    "# 5. Separación de Datos (Train/Val/Test)\n",
    "\n",
    "1. **Train dataset original**  \n",
    "   - Contiene todas las imágenes de entrenamiento (`split=\"train\"`).  \n",
    "   - Se usa para crear un **split interno** de entrenamiento y validación.\n",
    "\n",
    "2. **Split interno (train / val)**  \n",
    "   - 80% de las imágenes → subset de entrenamiento (`train_subset`)  \n",
    "   - 20% de las imágenes → subset de validación (`val_subset`)  \n",
    "   - Permite monitorear **loss y accuracy** durante el entrenamiento sin tocar el test final.\n",
    "\n",
    "3. **Test final (`val_dataset`)**  \n",
    "   - El split `val` de Imagenette se mantiene intacto y se usa como **test final**.  \n",
    "   - Garantiza que la evaluación final sea objetiva e independiente de los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e67a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores calculados en el EDA de Imagenette160\n",
    "imagenette_mean = [0.467, 0.448, 0.397]\n",
    "imagenette_std  = [0.230, 0.226, 0.228]\n",
    "\n",
    "normalize = transforms.Normalize(mean=imagenette_mean, std=imagenette_std)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.RandomResizedCrop(160, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),         \n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    \n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.5),\n",
    "    \n",
    "\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0))], p=0.5),\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "# 3. Transformaciones de Validación (sin aleatoriedad)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc2eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "val_size = int(0.2 * len(train_dataset))\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_subset.dataset.transform = train_transform\n",
    "val_subset.dataset.transform = val_transform\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader_internal = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "test_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader.dataset.transform = test_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a536adf",
   "metadata": {},
   "source": [
    "# 6. Funciones Train y Eval Genéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26e93bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, config, device, patience=5):\n",
    "    \"\"\"\n",
    "    Entrena un modelo con early stopping, logging en wandb y retorno del historial + mejor modelo.\n",
    "    \n",
    "    Args:\n",
    "        model: nn.Module, modelo a entrenar\n",
    "        train_loader: DataLoader del conjunto de entrenamiento\n",
    "        val_loader: DataLoader del conjunto de validación\n",
    "        config: dict con hiperparámetros (lr, optimizer, epochs, etc.)\n",
    "        device: \"cuda\" o \"cpu\"\n",
    "        patience: int, cantidad de epochs sin mejora para early stopping\n",
    "\n",
    "    Returns:\n",
    "        best_model: modelo con menor val_loss\n",
    "        history: diccionario con listas de train/val loss y accuracy\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if config[\"optimizer\"] == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    elif config[\"optimizer\"] == \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer {config['optimizer']} not supported\")\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc\": []\n",
    "    }\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        # ---- Entrenamiento ----\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # ---- Validación ----\n",
    "        model.eval()\n",
    "        val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, preds = outputs.max(1)\n",
    "                correct_val += preds.eq(labels).sum().item()\n",
    "                total_val += labels.size(0)\n",
    "\n",
    "        val_loss /= total_val\n",
    "        val_acc = correct_val / total_val\n",
    "\n",
    "        # ---- Guardar en historial ---\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        # ---- Log en W&B ----\n",
    "        wandb.log({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"epoch\": epoch\n",
    "        })\n",
    "\n",
    "        print(f\"[{epoch+1}/{config['epochs']}] \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # ---- Early stopping y guardar mejor modelo ----\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()  # guarda pesos\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"⏹ Early stopping en epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # ---- Devolver mejor modelo ----\n",
    "    best_model = model\n",
    "    if best_model_state is not None:\n",
    "        best_model.load_state_dict(best_model_state)\n",
    "\n",
    "    return best_model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c38f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples\n",
    "\n",
    "    return {\"loss\": avg_loss, \"accuracy\": accuracy}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b0051",
   "metadata": {},
   "source": [
    "# 7. Modelo Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef32c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBaseline(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNNBaseline, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # Bloque 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            # Bloque 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # Bloque 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(128 * 20 * 20, 128),  # De 256 a 128\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.3),                \n",
    "        nn.Linear(128, num_classes)\n",
    "    )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "from torchinfo import summary\n",
    "model = CNNBaseline(num_classes=10).to(device)\n",
    "summary(model, input_size=(1, 3, 160, 160))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d24ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"bayes\",  \n",
    "    \"name\": \"cnn_baseline_sweep\",\n",
    "    \"metric\": {\"name\": \"val_acc\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\"values\": [1e-2, 1e-3, 1e-4,]},\n",
    "        \"batch_size\": {\"values\": [64, 128]},\n",
    "        \"optimizer\": {\"values\": [\"Adam\", \"SGD\"]},\n",
    "        \"dropout1\": {\"values\": [0.1, 0.2]},\n",
    "        \"dropout2\": {\"values\": [0.2, 0.3, 0.4]},\n",
    "        \"dropout3\": {\"values\": [0.3, 0.4, 0.5]},\n",
    "        \"epochs\": {\"value\": 15}  # fija 15 epochs, early stopping lo limitará\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aca795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_train():\n",
    "    # Config de W&B\n",
    "    wandb.init(\n",
    "        project=\"imagenette\",\n",
    "        config=wandb.config,\n",
    "        settings=wandb.Settings(console=\"off\")  \n",
    "    )    \n",
    "    config = wandb.config\n",
    "    \n",
    "    # Ajustamos el modelo con los dropout del sweep\n",
    "    class CNNBaselineSweep(nn.Module):\n",
    "        def __init__(self, num_classes=10):\n",
    "            super().__init__()\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, 3, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Dropout(config.dropout1),\n",
    "                \n",
    "                nn.Conv2d(32, 64, 3, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Dropout(config.dropout2),\n",
    "                \n",
    "                nn.Conv2d(64, 128, 3, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Dropout(config.dropout3)\n",
    "            )\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(128*20*20, 128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(config.dropout3),\n",
    "                nn.Linear(128, num_classes)\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            x = self.classifier(x)\n",
    "            return x\n",
    "\n",
    "    model = CNNBaselineSweep(num_classes=10).to(device)\n",
    "    \n",
    "    train_loader_sweep = DataLoader(train_subset, batch_size=config.batch_size, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_loader_sweep   = DataLoader(val_subset, batch_size=config.batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    \n",
    "    # Entrenar\n",
    "    best_model, history = train_model(model, train_loader_sweep, val_loader_sweep, config, device, patience=3)\n",
    "\n",
    "    tmp_path = f\"best_model_{wandb.run.id}.pth\"\n",
    "    torch.save(best_model.state_dict(), tmp_path)\n",
    "\n",
    "    artifact = wandb.Artifact(\n",
    "        name=f\"cnn_baseline_sweep_{wandb.run.id}\",\n",
    "        type=\"model\",\n",
    "        description=f\"Mejor modelo CNNBaseline del sweep con lr={config.lr}, dropout1={config.dropout1}, dropout2={config.dropout2}, dropout3={config.dropout3}, batch_size={config.batch_size}\"\n",
    "    )\n",
    "    artifact.add_file(tmp_path)\n",
    "    wandb.log_artifact(artifact)\n",
    "\n",
    "    wandb.finish()  # opcional\n",
    "\n",
    "    return best_model, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247a47e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esto genera muchos prints! por lo que el output fue borrado. \n",
    "sweep_id = wandb.sweep(sweep_config, project=\"imagenette\")\n",
    "wandb.agent(sweep_id, function=sweep_train, count=25)  # count = número de combinaciones a probar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c729dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperar la mejor run del sweep (la que tuvo mejor val_acc)\n",
    "entity = \"kidnixt-ort\"     \n",
    "project = \"imagenette\"      \n",
    "sweep_id = \"yt8hz9zs\"  \n",
    "\n",
    "api = wandb.Api()\n",
    "sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "\n",
    "# Esto devuelve la mejor run según la métrica objetivo del sweep\n",
    "best_run = sweep.best_run()\n",
    "\n",
    "print(f\"🏆 Mejor run: {best_run.name} ({best_run.id})\")\n",
    "print(\"Config:\", best_run.config)\n",
    "print(\"Summary:\", dict(best_run.summary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46aa258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entrenamos con esos hiperparámetros, luego esto se obtendra como un artifact de wandb.\n",
    "class CNNBaselineBestParams(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNNBaselineBestParams, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # Bloque 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # Bloque 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # Bloque 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(128 * 20 * 20, 128),  # De 256 a 128\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.4),                \n",
    "        nn.Linear(128, num_classes)\n",
    "    )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "from torchinfo import summary\n",
    "model = CNNBaselineBestParams(num_classes=10).to(device)\n",
    "summary(model, input_size=(1, 3, 160, 160))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea6754",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = best_run.config['batch_size']\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader_internal = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "LR = best_run.config['lr']\n",
    "OPTIMIZER = best_run.config['optimizer']\n",
    "EPOCHS = best_run.config['epochs']\n",
    "config = {\n",
    "    \"lr\": LR,\n",
    "    \"optimizer\": OPTIMIZER,\n",
    "    \"epochs\": EPOCHS\n",
    "}\n",
    "\n",
    "run_name = \"baseline-bestparams using \" + best_run.name\n",
    "tags = [\"baseline\", \"bestparams\"]  # puedes usar [\"baseline\"] para el otro caso\n",
    "\n",
    "wandb.init(\n",
    "    project=\"imagenette\",\n",
    "    name=run_name,\n",
    "    tags=tags,\n",
    "    config=config  # puedes incluir lr, optimizer, etc.\n",
    ")\n",
    "\n",
    "# Re-inicializamos el modelo\n",
    "model = CNNBaselineBestParams(num_classes=10).to(device)\n",
    "best_model, history = train_model(model, train_loader, val_loader_internal, config, device, patience=3)\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286d7227",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Número de epochs\n",
    "epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "# ---- Loss ----\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# ---- Accuracy ----\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, history[\"train_acc\"], label=\"Train Acc\")\n",
    "plt.plot(epochs, history[\"val_acc\"], label=\"Val Acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Train vs Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0354fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = evaluate(best_model, test_loader, device)\n",
    "\n",
    "print(\"📊 Resultados en Test:\")\n",
    "print(f\"Loss: {results_test['loss']:.4f}\")\n",
    "print(f\"Accuracy: {results_test['accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d897ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"bayes\",  \n",
    "    \"name\": \"cnn_baseline_TEST\",\n",
    "    \"metric\": {\"name\": \"val_acc\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\"values\": [1e-2, 1e-3, 1e-4,]},\n",
    "        \"batch_size\": {\"values\": [64, 128]},\n",
    "        \"optimizer\": {\"values\": [\"Adam\", \"SGD\"]},\n",
    "        \"dropout1\": {\"values\": [0.1, 0.2]},\n",
    "        \"dropout2\": {\"values\": [0.2, 0.3, 0.4]},\n",
    "        \"dropout3\": {\"values\": [0.3, 0.4, 0.5]},\n",
    "        \"epochs\": {\"value\": 15}  # fija 15 epochs, early stopping lo limitará\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071b853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"imagenette\")\n",
    "wandb.agent(sweep_id, function=sweep_train, count=2)  # count = número de combinaciones a probar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528d1053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "file_path = \"pyproject.toml\"\n",
    "\n",
    "# Asegurarnos que no haya una sesión previa viva\n",
    "wandb.finish()\n",
    "\n",
    "print(f\"✅ Existe el archivo?: {os.path.exists(file_path)}\")\n",
    "print(f\"📏 Tamaño: {os.path.getsize(file_path) / 1_000_000:.2f} MB\")\n",
    "\n",
    "# Nuevo run completamente independiente\n",
    "run = wandb.init(project=\"imagenette\", job_type=\"manual-upload\", reinit=True)\n",
    "\n",
    "artifact = wandb.Artifact(\n",
    "    name=\"keje\",\n",
    "    type=\"dataset\",\n",
    "    description=\"Subida manual de modelo preentrenado\"\n",
    ")\n",
    "\n",
    "artifact.add_file(file_path, name=\"best_model_manual.pth\")\n",
    "print(\"📎 Archivo agregado al artifact.\")\n",
    "\n",
    "artifact.save()\n",
    "wandb.log_artifact(artifact)\n",
    "\n",
    "\n",
    "run.finish()\n",
    "print(\"✅ Finalizado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c527ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import wandb\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time # Importar time, aunque la espera principal es con artifact.wait()\n",
    "\n",
    "def model(training_data: float) -> float:\n",
    "    \"\"\"Simulación de modelo.\"\"\"\n",
    "    return training_data * 2 + random.uniform(-1, 1)\n",
    "\n",
    "# Simulación de pesos y ruido\n",
    "weights = random.random()\n",
    "noise = random.random() / 5\n",
    "\n",
    "# Hiperparámetros y configuración\n",
    "config = {\n",
    "    \"epochs\": 10,\n",
    "    \"learning_rate\": 0.01,\n",
    "}\n",
    "\n",
    "# Definir la ruta local del archivo del modelo (usando Path para robustez en Windows)\n",
    "PATH = Path(\"model.txt\") \n",
    "\n",
    "# 1. Limpieza: Asegúrate de que no haya un archivo anterior que cause conflictos.\n",
    "if PATH.exists():\n",
    "    os.remove(PATH)\n",
    "    \n",
    "print(f\"El archivo del modelo se guardará en: {PATH.resolve()}\")\n",
    "\n",
    "# Usar el manejador de contexto para inicializar y cerrar W&B runs\n",
    "with wandb.init(project=\"imagenette\", entity=\"kidnixt-ort\", config=config) as run:\n",
    "    \n",
    "    # Simulación de entrenamiento\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        xb = weights + noise\n",
    "        yb = weights + noise * 2\n",
    "\n",
    "        y_pred = model(xb)\n",
    "        loss = (yb - y_pred) ** 2\n",
    "\n",
    "        print(f\"epoch={epoch}, loss={y_pred}\")\n",
    "        run.log({\"epoch\": epoch, \"loss\": loss})\n",
    "\n",
    "    # 2. Guarda el modelo localmente\n",
    "    # Usamos str(PATH) para asegurarnos de que el método open() funcione correctamente\n",
    "    with open(str(PATH), \"w\") as f:\n",
    "        f.write(str(weights)) \n",
    "\n",
    "    # 3. Define y añade el Artifact\n",
    "    model_artifact_name = \"model-demo\"\n",
    "    artifact = wandb.Artifact(\n",
    "        name=model_artifact_name, \n",
    "        type=\"model\", \n",
    "        description=\"My trained model\"\n",
    "    )\n",
    "    # Añadir el archivo local al Artifact\n",
    "    artifact.add_file(local_path=str(PATH))\n",
    "    \n",
    "    # 4. REGISTRA el Artifact con el run (Inicia la subida asíncrona)\n",
    "    run.log_artifact(artifact)\n",
    "    \n",
    "    # 5. ¡Paso CLAVE para Windows! Esperar a que la subida termine.\n",
    "    print(\"\\nEsperando a que los artifacts terminen de subir antes de finalizar el run...\")\n",
    "    try:\n",
    "        # Esto bloquea la ejecución hasta que la subida del artifact finaliza.\n",
    "        artifact.wait() \n",
    "        print(\"Subida de artifacts finalizada exitosamente.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"\\n--- ERROR CRÍTICO DE SUBIDA ---\")\n",
    "        print(f\"W&B falló al subir el Artifact (ValorError): {e}\")\n",
    "        print(\"Esto sugiere un problema de permisos o bloqueo de disco.\")\n",
    "        print(\"Intenta ejecutar el terminal/IDE como Administrador.\")\n",
    "\n",
    "print(f\"\\nProceso finalizado. Comprueba el Artifact en tu dashboard de W&B.\")\n",
    "\n",
    "# Limpieza opcional: si quieres borrar el archivo local después de la subida\n",
    "# os.remove(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a63461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artifact name specifies the specific artifact version within our team's project\n",
    "\n",
    "TEAM_ENTITY = \"kidnixt-ort\"  # Your W&B team or username\n",
    "PROJECT = \"imagenette\"       # Your W&B project name\n",
    "artifact_name = f'{TEAM_ENTITY}/{PROJECT}/{model_artifact_name}'\n",
    "print(\"Artifact name: \", artifact_name)\n",
    "\n",
    "REGISTRY_NAME = \"Model\" # Name of the registry in W&B\n",
    "COLLECTION_NAME = \"DemoModels\"  # Name of the collection in the registry\n",
    "\n",
    "# Create a target path for our artifact in the registry\n",
    "target_path = f\"wandb-registry-{REGISTRY_NAME}/{COLLECTION_NAME}\"\n",
    "print(\"Target path: \", target_path)\n",
    "\n",
    "run = wandb.init(entity=TEAM_ENTITY, project=PROJECT)\n",
    "model_artifact = run.use_artifact(artifact_or_name=artifact_name, type=\"model\")\n",
    "run.link_artifact(artifact=model_artifact, target_path=target_path)\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebebe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.path.exists(\"best_model_9mrxi4y8.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e304bc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- VARIABLES DE ENTORNO DE W&B ---\n",
      "WANDB_CACHE_DIR (Caché global): No definida\n",
      "WANDB_DATA_DIR (Archivos temporales): No definida\n",
      "WANDB_CONFIG_DIR (Configuración/Login): No definida\n",
      "-----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>c:\\Users\\kidni\\Desktop\\imagenette\\wandb\\offline-run-20250930_194004-4wno5jec</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RUTAS DE UN RUN DE W&B (DRY RUN) ---\n",
      "Directorio de logs del Run (WANDB_DIR): C:\\Users\\kidni\\Desktop\\imagenette\\wandb\\offline-run-20250930_194004-4wno5jec\\files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kidni\\AppData\\Local\\Temp\\ipykernel_17976\\146756198.py\", line 21, in <module>\n",
      "    print(f\"Directorio de caché de Artifacts: {Path(wandb.util.get_cache_dir()).resolve()}\")\n",
      "AttributeError: module 'wandb.util' has no attribute 'get_cache_dir'\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br><code>wandb sync c:\\Users\\kidni\\Desktop\\imagenette\\wandb\\offline-run-20250930_194004-4wno5jec<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\offline-run-20250930_194004-4wno5jec\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se pudo inicializar W&B para verificar rutas: module 'wandb.util' has no attribute 'get_cache_dir'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "\n",
    "# Imprime las variables de entorno que controlan las rutas\n",
    "print(\"--- VARIABLES DE ENTORNO DE W&B ---\")\n",
    "print(f\"WANDB_CACHE_DIR (Caché global): {os.getenv('WANDB_CACHE_DIR', 'No definida')}\")\n",
    "print(f\"WANDB_DATA_DIR (Archivos temporales): {os.getenv('WANDB_DATA_DIR', 'No definida')}\")\n",
    "print(f\"WANDB_CONFIG_DIR (Configuración/Login): {os.getenv('WANDB_CONFIG_DIR', 'No definida')}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Si tienes un run activo, esta función te diría dónde están los logs y archivos de ese run específico.\n",
    "# Puedes ver dónde se guardarían los logs si inicias un run:\n",
    "try:\n",
    "    import wandb\n",
    "    with wandb.init(project=\"check_paths\", mode=\"dryrun\") as run: # dryrun previene la subida real\n",
    "        print(\"--- RUTAS DE UN RUN DE W&B (DRY RUN) ---\")\n",
    "        # El directorio donde se guardan los archivos de LOG/RUN (por defecto, en el directorio del script)\n",
    "        print(f\"Directorio de logs del Run (WANDB_DIR): {Path(run.dir).resolve()}\")\n",
    "        # El directorio de caché de Artifacts usado:\n",
    "        print(f\"Directorio de caché de Artifacts: {Path(wandb.util.get_cache_dir()).resolve()}\")\n",
    "        print(\"-\" * 35)\n",
    "except Exception as e:\n",
    "    print(f\"No se pudo inicializar W&B para verificar rutas: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a780291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb version: 0.22.1\n",
      "wandb path: c:\\Users\\kidni\\Desktop\\imagenette\\.venv\\lib\\site-packages\\wandb\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "print(\"wandb version:\", wandb.__version__)\n",
    "print(\"wandb path:\", wandb.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6336af78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: WANDB_DATA_DIR ha sido forzada a: C:\\wandb_temp\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\kidni\\Desktop\\imagenette\\wandb\\run-20250930_194212-xysigngr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kidnixt-ort/imagenette/runs/xysigngr' target=\"_blank\">winter-dust-96</a></strong> to <a href='https://wandb.ai/kidnixt-ort/imagenette' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kidnixt-ort/imagenette' target=\"_blank\">https://wandb.ai/kidnixt-ort/imagenette</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kidnixt-ort/imagenette/runs/xysigngr' target=\"_blank\">https://wandb.ai/kidnixt-ort/imagenette/runs/xysigngr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, loss=0.4514\n",
      "epoch=1, loss=0.0759\n",
      "epoch=2, loss=0.0019\n",
      "epoch=3, loss=0.4999\n",
      "epoch=4, loss=0.1806\n",
      "epoch=5, loss=0.6132\n",
      "epoch=6, loss=0.4569\n",
      "epoch=7, loss=0.0495\n",
      "epoch=8, loss=0.0341\n",
      "epoch=9, loss=0.0121\n",
      "\n",
      "Esperando a que el Artifact termine de subir...\n",
      "Subida de Artifact finalizada exitosamente.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>▆▂▁▇▃█▆▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.01213</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-dust-96</strong> at: <a href='https://wandb.ai/kidnixt-ort/imagenette/runs/xysigngr' target=\"_blank\">https://wandb.ai/kidnixt-ort/imagenette/runs/xysigngr</a><br> View project at: <a href='https://wandb.ai/kidnixt-ort/imagenette' target=\"_blank\">https://wandb.ai/kidnixt-ort/imagenette</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250930_194212-xysigngr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Proceso finalizado. Verifica el Artifact en W&B.\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "import wandb\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# =======================================================\n",
    "# PASO FINAL Y CLAVE: FORZAR LA VARIABLE EN EL ENTORNO DE PYTHON\n",
    "# =======================================================\n",
    "# 1. Creamos y forzamos la variable WANDB_DATA_DIR. \n",
    "# Esto obliga a W&B a usar C:\\wandb_temp en lugar de la ruta bloqueada de AppData.\n",
    "# ¡Asegúrate de que la carpeta C:\\wandb_temp exista!\n",
    "os.environ[\"WANDB_DATA_DIR\"] = \"C:\\\\wandb_temp\" \n",
    "print(f\"DEBUG: WANDB_DATA_DIR ha sido forzada a: {os.environ['WANDB_DATA_DIR']}\")\n",
    "# =======================================================\n",
    "\n",
    "def model(training_data: float) -> float:\n",
    "    \"\"\"Model simulation for demonstration purposes.\"\"\"\n",
    "    return training_data * 2 + random.uniform(-1, 1) \n",
    "\n",
    "# Simulación de pesos y ruido\n",
    "weights = random.random()\n",
    "noise = random.random() / 5\n",
    "\n",
    "# Hyperparameters and configuration\n",
    "config = {\n",
    "    \"epochs\": 10,\n",
    "    \"learning_rate\": 0.01,\n",
    "}\n",
    "\n",
    "# 2. Ruta local del modelo\n",
    "PATH = Path(\"model.txt\") \n",
    "if PATH.exists():\n",
    "    os.remove(PATH)\n",
    "\n",
    "# Usar contexto manager para W&B\n",
    "with wandb.init(project=\"imagenette\", entity=\"kidnixt-ort\", config=config) as run:\n",
    "    \n",
    "    # Simulación de entrenamiento\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        xb = weights + noise\n",
    "        yb = weights + noise * 2\n",
    "\n",
    "        y_pred = model(xb) \n",
    "        loss = (yb - y_pred) ** 2\n",
    "\n",
    "        print(f\"epoch={epoch}, loss={loss:.4f}\")\n",
    "        run.log({\"epoch\": epoch, \"loss\": loss})\n",
    "\n",
    "    # 3. Guarda el modelo localmente\n",
    "    with open(str(PATH), \"w\") as f:\n",
    "        f.write(str(weights)) \n",
    "\n",
    "    # 4. Define y añade el Artifact\n",
    "    model_artifact_name = \"model-demo\"\n",
    "    artifact = wandb.Artifact(\n",
    "        name=model_artifact_name, \n",
    "        type=\"model\", \n",
    "        description=\"My trained model\"\n",
    "    )\n",
    "    artifact.add_file(local_path=str(PATH))\n",
    "    \n",
    "    # 5. REGISTRA el Artifact\n",
    "    run.log_artifact(artifact)\n",
    "    \n",
    "    # 6. Espera la subida (Mantenemos el try/except)\n",
    "    print(\"\\nEsperando a que el Artifact termine de subir...\")\n",
    "    try:\n",
    "        artifact.wait() \n",
    "        print(\"Subida de Artifact finalizada exitosamente.\")\n",
    "    except ValueError as e:\n",
    "        # Si esto falla ahora, es un bloqueo del sistema de archivos extremadamente raro, \n",
    "        # pero al menos la ruta de staging será la correcta.\n",
    "        print(f\"\\n--- ERROR PERSISTENTE DE SUBIDA ---\")\n",
    "        print(f\"W&B falló al subir Artifact (ValorError): {e}\")\n",
    "        print(\"Esto sugiere un bloqueo de disco/antivirus. Por favor, intenta la Solución Offline.\")\n",
    "        \n",
    "print(f\"\\nProceso finalizado. Verifica el Artifact en W&B.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imagenette-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
